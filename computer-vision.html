<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Computer Vision</title>
		<link rel="icon" type="image/png" href="images/favicon.png"/>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
                            <li><a href="data-access.html">Data Access</a></li>
                            <li><a href="ai-bias.html">AI Bias</a></li>
                            <li><a href="filter-bubble.html">Filter Bubble</a></li>
                            <li><a href="computer-vision.html">Computer Vision</a></li>
                            <li><a href="fake-news.html">Fake News</a></li>
                            <li><a href="ai-human-interfaces.html">AI Human Interfaces</a></li>
                            <li><a href="about.html">About</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Computer Vision</h1>
							<span class="image main"><img src="images/computer-vision.jpg" alt="" height="500"/></span>
							<b><h2>What is Computer Vision?</h2></b>
							<p>Plainly speaking, Computer Vision falls under the broader and larger field of Artificial Intelligence (AI), and is currently one of the fastest growing and most focus-centered subfields of AI. A simple and easy way to define computer vision would be the ability for a computer to see, in the same sense in which we humans can see. But the action of “seeing” is not as simple as it may sound. To see something does not just mean to sight a scene or an image in front of you, but rather it deals with understanding what’s happening in the image in front of you, as to make sense of the context of the image or video. Take the image below as an example:  </p>
							<img src="images/milk-pouring.jpg" alt="" width="500" height="300"/>
							<br>
							<p>By just quickly glimpsing at the image, you have immediately noticed the essential parts of the overall image, such as a person (you recognized their gender as well), a milk can and a cup. Your eyes, along with your brain, managed to see and process all these parts in a single image with a blink of an eye, without any effort or hard work required from you. More deeply, you also understand what is happening in the image. You can interpret from the image that a man (knowing from the shirt and arms that it’s a male and not a female) is holding a can of milk and is pouring it into a cup. This was easy for you to understand because you have seen such a scene many times before, from your own personal experience of pouring milk into a cup or even just having seen someone do it in front of you, therefore this scene is familiar to you and you were able to easily understand what is happening in the image (the context of the image).</p>
							<p>Although this seems easy and effortlessly to you, it remains a very hard and complex task to make a computer understand the image the same way you just did, and at the same speed. It is much more difficult to the computer than you think, because the computer sees this image as nothing more than just an array of numbers (which represent the pixels that make up the image). This is how the computer sees the above image when you feed it to the program as an input:</p>
							<img src="images/pixels.jpg" alt="" width="600" height="300"/>
							<br>
							<p>You can clearly understand now why it is such a difficult task for the computer to understand and make sense of the image, as it only “sees” the image a huge list of numbers. This is the issue that Computer Vision is solving and thanks to the many decades of research in this subfield of Artificial Intelligence, beginning particularly in the 1960’s, computer scientists and engineers managed to come up with a rule of algorithms that help the computer “see” the image and even understand it, by extracting the meaning or context from the image in the exact same way in which we humans do. </p>
							<b><h2>How does the computer actually “see” the image?</h2></b>
							<p>Before explaining in practice how it works, we must address the following necessary terms: Machine Learning, Deep Learning, and Artificial Neural Networks. Although these three terms are interconnected, they differ in one way or another. Machine Learning is a subset of Artificial Intelligence in which a program is able to “learn” through experience. This means that a computer program can improve its algorithm as it is fed huge amounts of data and learns from its previous mistakes without the direct interference or assistance from a human programmer. Therefore, it is automatic and does not rely on task-specific rules for self-improvement. For example, if an AI program with the task of detecting fraud banking transactions was built, this program must be given tons of transactions to handle as training data in order for it to learn. The training data should equally vary between real and fraud transactions. The program is then given the correct answer at the end of each decision, and as a result, after a long time of trial and error processes, the program eventually self-improves by modifying its algorithm and therefore increases its accuracy in decisions. This is exactly what Machine Learning is. </p>
							<p>Before defining Deep Learning, we must first explain Artificial Neural Networks. Artificial Neural Networks (ANN) is a computing system that mimics our human brain’s biological neural network (just on a smaller and simpler scale) in the way our brain’s neurons process information (the input), pass it on to the next neuron, and at the end give a final result (the output).</p>
							<img src="images/neuron.jpg" alt="" width="500" height="400"/>
							<br>
							<p>The human’s Biological Neural Network is in charge of our reasoning, decision-making, and learning processes, by which we learn about something the more we see it, hear it, or feel it (basically anything we come across). ANN’s artificial neurons process and pass information in the same sense in which our brain’s neurons transmits information as signals between each other. In an ANN, groups of neurons form various different layers, in which each layer is responsible for a specific action or function. Each layer processes the piece of data that it has received as input from a previous neuron, and then passes it on to the next neuron on the second layer and so on, all the way until the final outcome (the result) is processed  by the last layer in the network and is given in the form of a decision. Similar to how a new-born baby discovers the world around it by trying to process as much information as possible, ANN improves as it gets fed with more and more data. </p>
							<p>Now that we have defined what an Artificial Neural Network is, we can explain what Deep Learning means. Deep Learning, as its name indicates, deals with a computer program’s ability to “learn” and improve through receiving large chunks of data for decision-making in many different ways. The “deep” in Deep Learning indicates the use of multiple layers in the Artificial Neural Network, which therefore allows the program to extract much more complex and sophisticated features, such as classifying an image as either showing a dog or a cat. The more layers there are in the ANN, the deeper the learning process is, and as a result the smarter and more complex the program is. This definition may sound very similar to Machine Learning, since the three terms Machine Learning, Artificial Neural Network, and Deep Learning are interlinked and are constantly being used together. Yet, it is important here to understand the hierarchy when mentioning these three terms. Artificial Neural Network (ANN) functions as the backbone of Deep Learning. Therefore, ANN is a subset of Deep Learning. Deep Learning is a subset of Machine Learning, and Machine Learning, at last, is a direct subfield of Artificial Intelligence of which all three terms fall under.</p>
							<img src="images/difference-in-terms.jpg" alt="" width="600" height="300"/>
							<br>
							<br>
							<p>Now that we have defined these three main terms, it is easy to follow along with the explanation on how computers “see” an image. Computer Vision uses Deep Learning techniques, which essentially relies on Artificial Neural Networks. There are many different types of Artificial Neural Network, and among them is what is known as the Convolutional Neural Network (CNN or ConvNet), which is what Computer Vision uses and what makes it so sophisticated and smart. CNN is a deep neural network, which means that it is an Artificial Neural Network with deep layers (multiple layers), that is why it is considered a Deep Learning approach. What make Convolutional Neural Networks so practical and special is that they do not need to have the classification features hard coded by the programmer or engineer. Rather, with enough training data being fed into the AI program, the CNN can learn to choose the most suitable features and characteristics in images of different categories, such as extracting the unique and most obvious features in a cat, a dog, a car, a human, a dolphin, a traffic light, a building, etc.</p>
							<p>The entire process of how an image is taken in by the AI program and giving out a prediction as a result can be simplified and summarized into 4 main stages or steps:</p>
								<ol>
									<li>Gather a large dataset of images for training the AI program’s model. The images should be labeled and classified, meaning that that there should be text that annotates each image as to what category it belongs to (a dog, a cat, a car, a ship, a tree, an airplane, etc.), in order for the program to begin learning to extract the difference between various classifications and categories.</li>
									<br>
									<img src="images/classification.jpg" alt="" width="600" height="300"/>
									<br>
									<br>
									<br>
									<li>The CNN has various layers of neurons, as explained before, and each artificial neuron has a “weight” that is assigned to it which indicates its importance (the weight variable is a number between 0 and 1). Obviously, the higher the weight of a neuron the more important this neuron’s function and decision is for the rest of the process. Weights are initially put as random numbers (literally random numbers), in order for the AI program to adjust its weights all by itself by raising the weights of the neurons that affect the accuracy of the final decision and lowering the weights of those that are not as important (such as the unnecessary background areas of images in which a cat is sitting in a park for example). This adjusting of the weights improves the accuracy in finding the most important features and characteristic in an image. Along with the weights, another important variable is set between the connections of two neurons known as the “bias”, which delays the activation of neurons that are less important for the final decision. Both these variables play an essential role in the accuracy of the AI program’s decision-making.</li>
									<img src="images/weights.jpg" alt="" width="500" height="400"/>
									<br>
									<br>
									<li>Left alone for a period of time (usually hours), the CNN trains itself to become smarter and more accurate by using a trial and error process, by which the AI model compares its prediction with the actual answer known as the “ground truth”. Each time the CNN makes a decision, it compares its answer with the correct label of the image from the training data in order to calculate how accurate its prediction was. Based on how high or how low the prediction was (the prediction answer is a number ranging between 0 and 1 which indicates its percentage out of 100% when multiplied by 100), the CNN model adjusts its weights and biases to get a better result for the next image. This process of having the model go back and modify its own weights and biases is known as Backpropagation.</li>
									<br>
									<br>
									<img src="images/backpropagation.jpg" alt="" width="500" height="400"/>
								</ol> 
								
							<p>After having trained the CNN enough, the AI program is able to take in any new (unlabeled) image for making a decision. It is important to separate the training images from the new images, in order to test the actual accuracy of the program when it is given an image that it hasn’t seen before. Most importantly, the new images must be in the range of the categories and classifications of images that it was trained with. For example, if the CNN model was trained entirely with dog and cat images, it will definitely be unable to process an image of a laptop or anything other than a dog or cat. Therefore, it is important and essential to set a goal on what kind of images you want your AI program to be able to detect. Convolutional Neural Network lays at the core of every Computer Vision program used nowadays. This was a very brief and general explanation of how Convolutional Neural Networks function, just for the sake of simplicity.</p>
							<img src="images/summary.jpg" alt="" width="700" height="200"/>
							<br>
							<br>
							<b><h2>Different types of Computer Vision</h2></b>
							<p>Computer Vision is no longer is a single topic, as more researchers, engineers, and scientists contributed to the development of it, it eventually grew larger to a point where Computer Vision now has 6 subcategories. Those are: </p>
								<br>	
								<ol>
									<h3>Image Classification</h3>
									<li>Image Classification is what we have been talking about so far. As its name explains, it deals with classifying images into one of a number of categories. For example, an AI program that can categorize an image of a cat or dog as belonging to one of the two options is said to be an AI program that uses Image Classification. Image Classification does not need to strictly be two categories or two classification groups only, as they can be multiple categories such as the ones found in self-driving cars, since it obviously wouldn’t be enough for an autonomous car to only be able to tell the different between a human and a car. Rather, self-driving cars are trained with an enormous amount of dataset of images varying between pedestrians, cars, stop signs, sidewalks, trees, traffic light, buildings, bicycles, trucks, and many more.</li>
									<br>
									<img src="images/image-classification.jpg" alt="" width="600" height="300"/>
									<br>
									<br>
									<br>
									<h3>Object Detection</h3>
									<li>Object Detection is a combination of two things: Image Classification, which we just explained, and what is known as Object Localization. Object Localization simply means to locate an object in an image by surrounding the detected object with a box to indicate that this is an object. Keep in mind, Object Localization only means locating an object, as in determining its position or location in an image, and not classifying it (categorizing the object). This is where Image Classification kicks in. When Image Classification and Object Localization go hand-in-hand together, we get Object Detection (also referred to as Object Recognition). Object Detection basically is a combination of the two, in which we first locate the object (or multiple objects in the same image) by surrounding it with a colored box, and secondly adding the classification to it by annotating the surrounding box with a text to indicate what this object that we have located is. This is another technique that self-driving cars use in order to “see” various different objects around it, each with a description (classification).</li>
									<br>
									<img src="images/object-detection.jpg" alt="" width="600" height="300"/>
									<br>
									<br>
									<br>
									<h3>Object Tracking</h3>
									<li>Object Tracking is very similar to Object Detection, but rather is dedicated to videos, to track moving objects that change their location continuously. Think of a video in which a ball moves from one soccer player to the other. In such a case, Object Tracking would keep track of the where the ball moves throughout the video. This is again crucial and necessary for self-driving cars to keep tracking of cars and pedestrians as they move around the car continuously.</li>
									<br>
									<img src="images/object-tracking.jpg" alt="" width="600" height="300"/>
									<br>
									<br>
									<br>
									<h3>Semantic Segmentation</h3>
									<li>Semantic Segmentation is very similar to Object Detection, yet what makes Semantic Segmentation more professional is that instead of just drawing a simple square or rectangular box around the detected object, Semantic Segmentation draws the borders of the detector (the box) around the exact borders of the object. This means the box’s sides align with the object’s borders in order to clearly understand the shape of the detected object. The picture below shows the clear difference between Object Detection, which draws a box around the located object, and Semantic Segmentation, which allows the computer to understand where the borders and edges of all located objects are.</li>
									<img src="images/semantic-segmentation.jpg" alt="" width="600" height="300"/>
									<br>
									<br>
									<br>
									<h3>Instance Segmentation</h3>
									<li>Instance Segmentation is just a one-step more advanced approach to Semantic Segmentation. The main difference between Instance Segmentation and Semantic Segmentation is that in Semantic Segmentation, all the detected objects are treated as one object, therefore the computer sees all 4 persons as one person (one object), and that is why they are all colored by the AI program with the same color. Instance Segmentation solves this problem by handling each detected object as separate objects. As the example below shows, with the use of Instance Segmentation, the five persons in the image are each colored with a different color to handle each object as a separate one.</li>
									<img src="images/instance-segmentation.jpg" alt="" width="800" height="300"/>
								</ol>
						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="copyright">
								<li>&copy; Infinite AI team. All rights reserved</li><li>2020</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>